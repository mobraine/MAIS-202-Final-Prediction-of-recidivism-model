#import libraries  

from google.colab import drive
import os
import csv
import random
from string import ascii_letters
import heapq
from random import shuffle
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
import numpy as np

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import random


#runnng the code below
drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/your_project_folder/'  #change dir to your project folder

from google.colab import files
files.upload()

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json  # set permission

!kaggle datasets list --user "slonnadube"
!kaggle datasets download -d  slonnadube/recidivism-for-offenders-released-from-prison



!ls ./3-Year_Recidivism_for_Offenders_Rel

data_3_year = 0
data_less = 0

with open("./3-Year_Recidivism_for_Offenders_Rel/3-Year_Recidivism_for_Offenders_Released_from_Prison_in_Iowa_elaborated.csv") as csv_file:
  csv_reader = csv.reader(csv_file)
  colnames_3_year = next(csv_reader)
  colnames_3_year = np.array(colnames_3_year)
  colnames_3_year = colnames_3_year[2:]
  data_3_year = list(csv_reader)
  data_3_year = np.array(data_3_year)
  data_3_year = data_3_year[:,2:]#removing first two columns containing release year and recidivism year
  print (colnames_3_year)

with open("./3-Year_Recidivism_for_Offenders_Rel/prison_recidivists_with_recidivism_type_only.csv") as csv_file:
  csv_reader = csv.reader(csv_file)
  colnames_less = next(csv_reader)
  colnames_less = np.array(colnames_less)
  colnames_less = colnames_less[2:]
  data_less = list(csv_reader)
  data_less = np.array(data_less)
  data_less = data_less[:,2:]
  print (colnames_less)


#randomly select training, testing and validation dataset
shuffled_data_3_year = data_3_year.copy()
shuffled_data_less = data_less.copy()

random.shuffle(shuffled_data_3_year)
random.shuffle(shuffled_data_less)

x_train = shuffled_data_3_year[0 : 13010]
x_test = shuffled_data_3_year[13011 : 20817]
x_validation = shuffled_data_3_year[20818 : 26021]

#x_train = random.sample(data_3_year, 13010)

print(x_train.shape)
print(data_less.shape)

#examine if the data is random 
print(colnames_3_year)
print(x_train[0:30])
#print(data_3_year[0:10])

#Direct run to get prelimary results.

#first numerize each data point
#I will have an array of dicts, to store the occrruence of each type in each feature and try to ccalculate their probability

def numerize (x):
  array_dicts = []
  for i in range(10):# going from columm to column
    dict = {}
    for j in range (len(x)):#going from row to row
      type = x[j][i]
      if type in dict:
        dict[type] += 1
      else:
        dict[type] = 1
    array_dicts.append(dict)
  return array_dicts

def prob(x):
  array_prob = []
  array_dicts = numerize (x)
  for i in range (9):
    dict = {}
    for type in array_dicts[i]:
      prob_type = array_dicts[i][type] / len(x)
      dict[type] = prob_type
    array_prob.append(dict)
  return array_prob

prob_x_overall = prob(shuffled_data_3_year)
prob_x_train = prob(x_train)
prob_x_test = prob(x_test)
prob_x_validation = prob(x_validation)
#prob_x_train.pop("0.6744811683320523")
print (prob_x_train)
print (prob_x_test)
print (prob_x_validation)
print (prob_x_overall)

print (len(prob_x_test))

#now that we have prob for each type of each category, let's map the prob to their location in each record.
def vectorize (x,prob_x):
  x_clone = x.copy()
  for record in x_clone:
    for i in range(9):
      type = record[i] 
      if type in prob_x[i]:
        record[i] = prob_x[i][type]
      #print (prob_x[i][record[i]])
  return x_clone


x_train_vectorized = vectorize(x_train, prob_x_overall)#vectorize(x_train, prob_x_train)
x_train_vectorized = np.delete(x_train_vectorized,0,0)

x_test_vectorized = vectorize(x_test, prob_x_overall)#vectorize(x_test, prob_x_test)
x_validation_vectorized = vectorize(x_validation, prob_x_overall)
#for i in range(50):
 # print (x_test_vectorized[i])
 
#print (x_test_vectorized.shape)

# Run model LinearSVM
# specify classes recid

def class_getter(x):
  recid = []
  #recid = np.array(recid)
  for record in x:
    if record[9] == '1':
      recid.append(1)
    else:
      recid.append(0)
  return recid

y_train = class_getter(x_train_vectorized)
print(y_train)
#svm
svm_clf = svm.LinearSVC()
svm_clf.fit(x_train_vectorized, y_train)

#function that make an array of prediction and an array original recidivism
def svm_predict(original):
  prediction = []
  for record in original:
    copy = record
    copy = copy.astype(np.float64)
    copy = copy.reshape(1, -1)
    prediction.append(svm_clf.predict(copy)[0])
  return prediction

def original(ori):
  origin = [] 
  for record in ori:
    origin.append(int(record[9]))
  return origin

test_pred_svm = svm_predict(x_test_vectorized)
test_ori = original(x_test)

validation_pred_svm = svm_predict(x_validation_vectorized)
validation_ori = original(x_validation)


#see recall score
print ("Recall score of test set is " + str(recall_score(test_ori, test_pred_svm, average='macro')))
print ("Recall score of validation set is " + str(recall_score(validation_ori, validation_pred_svm, average='macro'))) 

#see accuracy
print("Accuracy score of test set is " + str(accuracy_score(test_ori, test_pred_svm)))
print("Accuracy score of validation set is " + str(accuracy_score(validation_ori, validation_pred_svm)))

#see precision score
print("Precision score of test set is " + str(precision_score(test_ori, test_pred_svm, average='macro')))
print("Precision score of validation set is " + str(precision_score(validation_ori, validation_pred_svm, average='macro'))) 

#Random Forest
rfc = RandomForestClassifier(n_estimators=100, max_depth=3, min_samples_split = 2, max_features = 10 ,random_state=0)
rfc.fit(x_train_vectorized, y_train)

def rfc_predict(original):
  prediction = []
  for record in original:
    copy = record
    copy = copy.astype(np.float64)
    copy = copy.reshape(1, -1)
    prediction.append(rfc.predict(copy)[0])
  return prediction
  
test_pred_rfc = rfc_predict(x_test_vectorized)
validation_pred_rfc = rfc_predict(x_validation_vectorized)

#see recall score
print ("Recall score of test set is " + str(recall_score(test_ori, test_pred_rfc, average='macro')))
print ("Recall score of validation set is " + str(recall_score(validation_ori, validation_pred_rfc, average='macro'))) 

#see accuracy
print("Accuracy score of test set is " + str(accuracy_score(test_ori, test_pred_rfc)))
print("Accuracy score of validation set is " + str(accuracy_score(validation_ori, validation_pred_rfc)))

#see precision score
print("Precision score of test set is " + str(precision_score(test_ori, test_pred_rfc, average='macro')))
print("Precision score of validation set is " + str(precision_score(validation_ori, validation_pred_rfc, average='macro'))) 


#To make my model even more useful, I decided to plot which factor affect recidivism the most significantly. The idea is
#I will created an array of recidivist, and for each category, I will compute their probability of each type.

#first concatenate all three sets to make the vectorized original 

#shuffled_data_3_year_vec = np.concatenate((x_train_vectorized, x_test_vectorized, x_validation_vectorized), axis=0)
#print (shuffled_data_3_year_vec.shape)
#print (shuffled_data_3_year_vec[0:10])

recid = []
for record in shuffled_data_3_year:
  if record[9] == '1':
    recid.append(record)

recid = np.array(recid)
prob_x_overall_recid = prob(recid)
print(prob_x_overall_recid)

#now plot each of these category's categories categories tyoe distribution
def getLabels(category):
  labels = []
  for key in category:
    labels.append(key)
  return labels

def getValues(category):
  values = []
  for key in category:
    values.append(category[key])
  return values


for i in range(9):
  labels = getLabels(prob_x_overall_recid[i])
  values = getValues(prob_x_overall_recid[i])
  df = pd.DataFrame({colnames_3_year[i]:labels, 'val':values})
  ax = df.plot.barh(x=colnames_3_year[i], y='val', rot=0)
  fig = ax.get_figure()
  file_name = colnames_3_year[i] + '.pdf'
  fig.savefig(file_name, bbox_inches='tight')
  #ax = df[['lab', 'val']].plot(figsize=(10,5))
#notice that the largest proportion in main supervising district is null, which means there is no such district? 




